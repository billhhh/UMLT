{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise â€“ Learning rate scheduling and optimisers\n",
    "--------------------------------------------------\n",
    "\n",
    "In this exercise, you will practice using learning rate scheduling and different optimisers. You will experiment with different parameters, compare results and identify an appropriate choice for the handwritten digit classification task. You will build on these skills in Assessment 2 and Assessment 3. \n",
    "\n",
    "We will use the MNIST dataset again, which is a good training dataset consisting of handwritten digits (0 to 9). Here is the code base from Module 3, which you already know and can use to complete the tasks. It is a good idea to read through it again now. You can keep the code where it is in your Jupyter notebook and build on top of it when coding the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add appropriate imports here\n",
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('figure', dpi=120) # set good resolution\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "print(X_train_full.shape)\n",
    "XSIZE, YSIZE = X_train_full.shape[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the train_test_split function to split the data into training and validation, given that a \n",
    "# separate hold-out testset has already been provided. We will use an 80/20 split for training/validation.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, test_size =0.2, stratify = y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Keras sequential model with the following setup (use code examples from main module)\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [XSIZE , YSIZE]))\n",
    "model.add(keras.layers.Dense(300, activation = \"relu\"))\n",
    "model.add(keras.layers.Dense(100, activation = \"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer=keras.optimizers.Adam(lr=0.01), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run fit\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=1000, \n",
    "                    validation_data=(X_valid, y_valid),verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Task: Write a function that builds and returns a dense neural network model, just like the one above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Task: Train a dense network using the optimisers RMSprop, Adam, Nadam and AdaMax. Use the function that you created just before to build the models. Save the models and their training histories for analysis in the next task. Use 30 epochs and the accuracy metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Task: Display the learning curves of training and validation accuracy of all optimiser runs in one single diagram. Practice unambiguous and organised visualisation, by labelling the axis, using a legend, title, axis limits and suitable marker and line styles. Make sure to choose the axis limits, such that the important parts of the data are clearly visible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Question: Interpret the accuracy results across the range of optimisers. Which one convergences fastest? Which one has the best result? Which one looks robust?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Task: So RMSprop gives the best result but is slow. Let's add a learning rate scheduling to improve this. Create a learning rate scheduler that implements the exponential decay from Equation 4.14. Implement the decay factor s and the initial learning rate eta0 as attributes of the function that implements the formula. Then, you can set these two parameters to different values before training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Task: In order to save epochs, you should create an early stopping callback. If the training converges early, the training will then stop early. Create an early stopping callback with patience 5 that monitors the validation accuracy and restores the best weights at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Task: Run the training again with the RMSprop optimiser and the learning rate callback and early stopping callback that you just created. Use and initial learning rate eta0 of 0.01, which is higher than the default constant learning rate of 0.001. This will speed up the early parts. Repeat the training with decay factors or 1, 3, 5, 7, 9, and 11 and save the model and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Task: Display the learning curves of training and validation accuracy of all runs in one single diagram. Practice unambiguous and organised visualisation, by labelling the axis, using a legend, title, axis limits and suitable marker and line styles. Make sure to choose the axis limits, such that the important parts of the data are clearly visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Question: Interpret the accuracy results across the range of decay rates. Which one convergences fastest? Which one has the best result? Which decay rate may be the best trade-off in your results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
