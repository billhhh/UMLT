{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 10 - Training Neural Networks (Learning rate scheduling and optimisers)\n",
    "\n",
    "Code for workshop 10.  This will use Keras (within tensorflow v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os, time\n",
    "import pandas as pd\n",
    "\n",
    "# Deep Learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# To plot nice figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "mpl.rc('figure', dpi=100)\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the versions are OK (both should be 2 or more)\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "We will use MNIST, which is a set of small images (28x28) that contain 10 digits - see below for class names and an example image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a built-in data for keras, so easily accessible\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how big it is\n",
    "print(X_train_full.shape)\n",
    "print(X_test.shape)\n",
    "n_total = X_train_full.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data appropriately (it starts with max of 255, but we want max of 1)\n",
    "# We will do this \"by hand\" here, but we could build a pipeline scaler for this instead\n",
    "# We also split the training set given to us into training and validation subsets\n",
    "#   The value of 5000 samples as the size of the validation set is an arbitrary choice\n",
    "X_test = X_test/255.0\n",
    "X_valid, X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0 \n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "class_names = np.array([ \"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\" ])\n",
    "\n",
    "# Inspect some aspects of the data (in general, you should play around with the data \n",
    "#                                   more than this to get a feel for it)\n",
    "# Check that scaled types are appropriate\n",
    "print(X_train.dtype)\n",
    "print(X_valid.dtype)\n",
    "# Look at first item\n",
    "print(class_names[y_train[0]])\n",
    "plt.imshow(X_train[0,:,:], cmap='gray')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the distribution of labels in the training, validation and test sets\n",
    "plt.hist(y_train)\n",
    "plt.show()\n",
    "plt.hist(y_valid)\n",
    "plt.show()\n",
    "plt.hist(y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network Code with Keras\n",
    "\n",
    "We will use the keras version built into tensorflow version 2.\n",
    "It is remarkably simple for building, training and evaluating networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some key parameters\n",
    "n_train = 300\n",
    "n_valid = 100\n",
    "# Define the number and size of hidden layers\n",
    "hiddensizes = [16, 32, 16]\n",
    "# Define the activation function to be used by hidden layers\n",
    "#actfn = \"relu\"\n",
    "actfn = \"elu\"\n",
    "# Optimiser and learning rate\n",
    "optimizer = keras.optimizers.SGD\n",
    "learningrate = 0.01   # SGD default value\n",
    "# Set size of batch and number of epochs\n",
    "batch_size = 32\n",
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a CNN\n",
    "def model_cnn_factory(hiddensizes, actfn, optimizer, learningrate=0):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(filters=hiddensizes[0], kernel_size=3, strides=1, activation=actfn, padding=\"same\", \n",
    "                                  input_shape=[28, 28, 1]))    # input layer goes into this 2D convolution\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=2))          # Pool (downsize)\n",
    "    for n in hiddensizes[1:-1]:\n",
    "        model.add(keras.layers.Conv2D(filters=n, kernel_size=3, strides=1, padding=\"same\", activation=actfn))  # 2nd Conv\n",
    "        model.add(keras.layers.MaxPooling2D(pool_size=2))          # Pool (downsize)\n",
    "    model.add(keras.layers.Conv2D(filters=hiddensizes[-1], kernel_size=3, strides=1, padding=\"same\", activation=actfn))  # 2nd Conv\n",
    "    model.add(keras.layers.Flatten())                          # unravel into a 1D vector\n",
    "    model.add(keras.layers.Dense(10, activation = \"softmax\"))  # always have 10 classes\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer(learning_rate=learningrate), metrics=[\"accuracy\"])   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional aside: the CNN can become a Fully Convolutional Network (FCN) by replacing the Flatten and Dense lines with\n",
    "#   model.add(keras.layers.Conv2D(filters=10, kernel_size=7, padding=\"valid\", activation=\"softmax\"))\n",
    "# This uses a kernel equal to the full image size (at this point) to generate a single output per filter \n",
    "#  which requires the convolution to be \"valid\" and not \"same\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dense_factory(hiddensizes, actfn, optimizer, learningrate):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape = [28, 28, 1]))    # always have same sized inputs\n",
    "    for n in hiddensizes:\n",
    "        model.add(keras.layers.Dense(n, activation = actfn))\n",
    "    model.add(keras.layers.Dense(10, activation = \"softmax\"))   # always have 10 classes\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer(learning_rate=learningrate), metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data to be shape [Nx, Ny, 1]  (previously 2D was fine, but for CNN we need depth too)\n",
    "X_train = X_train.reshape((-1, 28, 28, 1))\n",
    "X_valid = X_valid.reshape((-1, 28, 28, 1))\n",
    "X_test = X_test.reshape((-1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback - this is executed when fitting and will stop and restore best result\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='???', patience=5, restore_best_weights=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_all(hiddensizes, actfn, optimizer, learningrate, n_train, n_valid, n_epochs, batch_size, further_callbacks=[]):\n",
    "    if further_callbacks != []:\n",
    "        callbacks = further_callbacks\n",
    "    else:\n",
    "        callbacks = [early_stopping_cb]\n",
    "    model = model_???_factory(hiddensizes, actfn, optimizer, learningrate)\n",
    "    history = model.fit(X_train[:n_train,:,:,:], y_train[:n_train], epochs=n_epochs, callbacks = callbacks,\n",
    "                        validation_data=(X_valid[:n_valid,:,:,:], y_valid[:n_valid]))\n",
    "    max_val_acc = np.max(history.history['val_accuracy'])\n",
    "    return (max_val_acc, history, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valacc, history, model = do_all(hiddensizes, actfn, optimizer, learningrate, n_train, n_valid, n_epochs, batch_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    # Plot the results (shifting validation curves appropriately)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    n = len(history.history['accuracy'])\n",
    "    plt.plot(np.arange(0,n),history.history['accuracy'], color='orange')\n",
    "    plt.plot(np.arange(0,n),history.history['loss'],'b')\n",
    "    plt.plot(np.arange(0,n)+0.5,history.history['val_accuracy'],'r')  # offset both validation curves\n",
    "    plt.plot(np.arange(0,n)+0.5,history.history['val_loss'],'g')\n",
    "    plt.legend(['Train Acc','Train Loss','Val Acc','Val Loss'])\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 1) # set the vertical range to [0-1] \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually exploring the learning rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "res=[]\n",
    "for lr in [???]:\n",
    "    valacc, history, discard = do_all(hiddensizes, actfn, optimizer, lr*learningrate, n_train, n_valid, n_epochs, batch_size)\n",
    "    plot_history(history)\n",
    "    res += [[lr*learningrate,valacc]]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=np.array(res)\n",
    "plt.plot(res[:,0],res[:,1])\n",
    "plt.title('Accuracy vs Learning Rate')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Val Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of layers\n",
    "res=[]\n",
    "for n in [???]:\n",
    "    valacc, history, discard = do_all(hiddensizes[:n], actfn, optimizer, learningrate, n_train, n_valid, n_epochs, batch_size)\n",
    "    plot_history(history)\n",
    "    res += [[n,valacc]]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=np.array(res)\n",
    "plt.plot(res[:,0],res[:,1])\n",
    "plt.title('Accuracy vs Layers')\n",
    "plt.xlabel('Number of Layers')\n",
    "plt.ylabel('Val Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring optimizers\n",
    "\n",
    "Let us compare different optimizers on the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=[]\n",
    "for optimizer,lr in [[keras.optimizers.SGD,0.01], [???,???],  . . .  ]:\n",
    "    valacc, history, discard = do_all(hiddensizes, actfn, optimizer, lr, n_train, n_valid, n_epochs, batch_size)\n",
    "    plot_history(history)\n",
    "    res += [[valacc]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=np.array(res)\n",
    "plt.plot(res[:,1])\n",
    "plt.ylabel('Val Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Tasks\n",
    "\n",
    "Which one has the best final classifier? \n",
    "\n",
    "Which one converges fastest? \n",
    "\n",
    "Which learning curve is the smoothest?\n",
    "\n",
    "Which one would you choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring learning rate schedules\n",
    "\n",
    "Let us compare different learning rate schedules on the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scheduler functions\n",
    "def exp_schedule(epoch):\n",
    "    stretch=5\n",
    "    return 0.0000001 * (10**(epoch/stretch))\n",
    "\n",
    "\n",
    "early_stopping_cb2 = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=12, restore_best_weights=True) \n",
    "\n",
    "# Run with piecewise_const learning rate scheduler\n",
    "optimizer=keras.optimizers.Nadam\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exp_schedule)\n",
    "valacc, history, discard = do_all(hiddensizes, actfn, optimizer, learningrate, \n",
    "                                           n_train, n_valid, 50, batch_size,\n",
    "                                           [lr_scheduler,early_stopping_cb2])\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlr=exp_schedule(22)\n",
    "print(maxlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs=[]\n",
    "for n in range(50):\n",
    "        lrs = lrs + [exp_schedule(n)]\n",
    "plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scheduler functions\n",
    "def OneCycleSchedule(epoch):\n",
    "    maxepoch=20\n",
    "    if epoch<maxepoch/2:\n",
    "        lr=maxlr*(0.1 + (0.9*epoch/(maxepoch/2)))\n",
    "    elif epoch<maxepoch:\n",
    "        lr=maxlr*(1.9-0.9*epoch/(maxepoch/2))\n",
    "    else:\n",
    "        lr=0.1*maxlr\n",
    "    return lr\n",
    "\n",
    "\n",
    "early_stopping_cb3 = keras.callbacks.EarlyStopping(monitor='???', patience=6, restore_best_weights=True) \n",
    "\n",
    "# Run with piecewise_const learning rate scheduler\n",
    "optimizer=keras.optimizers.Nadam\n",
    "lr_scheduler2 = tf.keras.callbacks.LearningRateScheduler(OneCycleSchedule)\n",
    "valacc, history, discard = do_all(hiddensizes, actfn, optimizer, learningrate, \n",
    "                                           n_train, n_valid, n_epochs, batch_size,\n",
    "                                           [lr_scheduler2,early_stopping_cb3])\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs=[]\n",
    "for n in range(n_epochs):\n",
    "        lrs = lrs + [OneCycleSchedule(n)]\n",
    "plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Tasks: \n",
    "\n",
    "Write a new learning rate scheduler (copy from above and edit) that does a simple '1cycle' scheduling. Use the manual learning rate tests above *but with the Nadam optimizer* to choose a maximum learning rate. \n",
    "\n",
    "Manually experiment with the number of iterations for the upslope/down slope. \n",
    "\n",
    "Is the convergence faster than with a fixed learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
