{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 11 - Network architectures (CNN vs ResNET)\n",
    "\n",
    "Code for workshop 11.  This will use Keras (within tensorflow v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## 1. Initialise the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os, time\n",
    "import pandas as pd\n",
    "\n",
    "# Deep Learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# To plot nice figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "mpl.rc('figure', dpi=100)\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "# Check the versions are OK (both should be 2 or more)\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## 2. Load, inspect and prepare the data\n",
    "\n",
    "We will use MNIST, which is a set of small images (28x28) that contain 10 \n",
    "digits - see below for class names and an example image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a built-in data for keras, so easily accessible\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how big it is\n",
    "print(X_train_full.shape)\n",
    "print(X_test.shape)\n",
    "n_total = X_train_full.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data appropriately (it starts with max of 255, but we want\n",
    "# max of 1)\n",
    "# We will do this \"by hand\" here, but we could build a pipeline scaler \n",
    "# for this instead. We also split the training set given to us into \n",
    "# training and validation subsets \n",
    "# The value of 5000 samples as the size of the validation set is an \n",
    "# arbitrary choice\n",
    "X_test = X_test/255.0\n",
    "X_valid, X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0 \n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "class_names = np.array([ \"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\" ])\n",
    "\n",
    "# Inspect some aspects of the data (in general, you should play around \n",
    "# with the data more than this to get a feel for it)\n",
    "# Check that scaled types are appropriate\n",
    "print(X_train.dtype)\n",
    "print(X_valid.dtype)\n",
    "# Look at first item\n",
    "print(class_names[y_train[0]])\n",
    "plt.imshow(X_train[0,:,:], cmap='gray')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the distribution of labels in the training, validation \n",
    "# and test sets\n",
    "fig, sub = plt.subplots(1, 3)\n",
    "fig.set_size_inches(12,3)\n",
    "sub[0].hist(y_train)\n",
    "sub[0].set_ylabel('Count')\n",
    "sub[0].set_xlabel('Class')\n",
    "sub[0].set_title('y_train')\n",
    "sub[1].hist(y_valid)\n",
    "sub[1].set_xlabel('Class')\n",
    "sub[1].set_title('y_valid')\n",
    "sub[2].hist(y_test)\n",
    "sub[2].set_xlabel('Class')\n",
    "sub[2].set_title('y_test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data to be shape [Nx, Ny, 1]  (previously 2D was fine, but for \n",
    "#CNN we need depth too)\n",
    "X_train = X_train.reshape((-1, 28, 28, 1))\n",
    "X_valid = X_valid.reshape((-1, 28, 28, 1))\n",
    "X_test = X_test.reshape((-1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## 3. Define helper functions that build models in a parameterised way\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a CNN\n",
    "def model_cnn_factory(hiddensizes, actfn, optimizer, learningrate=0):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(filters=hiddensizes[0], kernel_size=3, \n",
    "                                  strides=1, activation=actfn, padding=\"same\", \n",
    "                                  input_shape=[28, 28, 1]))    \n",
    "                                  # input layer goes into this 2D convolution\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=2))     # Pool (downsize)\n",
    "    for n in hiddensizes[1:-1]:\n",
    "        model.add(keras.layers.Conv2D(filters=n, kernel_size=3, strides=1, \n",
    "                                      padding=\"same\", activation=actfn))  \n",
    "                                      # 2nd Conv\n",
    "        model.add(keras.layers.MaxPooling2D(pool_size=2)) # Pool (downsize)\n",
    "    model.add(keras.layers.Conv2D(filters=hiddensizes[-1], kernel_size=3, \n",
    "                                  strides=1, padding=\"same\", activation=actfn))  \n",
    "                                  # 2nd Conv\n",
    "    model.add(keras.layers.Flatten())          # unravel into a 1D vector\n",
    "    model.add(keras.layers.Dense(10, activation = \"softmax\"))  \n",
    "        # always have 10 classes\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "                  optimizer=optimizer(learning_rate=learningrate), metrics=[\"accuracy\"])   \n",
    "    return model\n",
    "\n",
    "# Optional aside: the CNN can become a Fully Convolutional Network (FCN) by \n",
    "# replacing the Flatten and Dense lines with model.add(keras.layers.\n",
    "# Conv2D(filters=10, kernel_size=7, padding=\"valid\", activation=\"softmax\"))\n",
    "# This uses a kernel equal to the full image size (at this point) to generate \n",
    "# a single output per filter which requires the convolution to be \"valid\" \n",
    "# and not \"same\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_evaluate(model, n_train, n_valid, n_epochs, batch_size, callbacks=[]):\n",
    "    history = model.fit(X_train[:n_train,:,:,:], y_train[:n_train], epochs=n_epochs, callbacks = callbacks,\n",
    "                        validation_data=(X_valid[:n_valid,:,:,:], y_valid[:n_valid]),verbose=1)\n",
    "    max_val_acc = np.max(history.history['val_accuracy'])\n",
    "    testres = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return (max_val_acc, testres[1], history, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    # Plot the results (shifting validation curves appropriately)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    n = len(history.history['accuracy'])\n",
    "    plt.plot(np.arange(0,n),history.history['accuracy'], color='orange')\n",
    "    plt.plot(np.arange(0,n),history.history['loss'],'b')\n",
    "    plt.plot(np.arange(0,n)+0.5,history.history['val_accuracy'],'r')  # offset both validation curves\n",
    "    plt.plot(np.arange(0,n)+0.5,history.history['val_loss'],'g')\n",
    "    plt.legend(['Train Acc','Train Loss','Val Acc','Val Loss'])\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 1) # set the vertical range to [0-1] \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## 4. Define common modelling parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some key parameters\n",
    "n_train = 3000\n",
    "n_valid = 1000\n",
    "# Define the number and size of hidden layers\n",
    "hiddensizes = [16, 32, 16]\n",
    "# Define the activation function to be used by hidden layers\n",
    "#actfn = \"relu\"\n",
    "actfn = \"elu\"\n",
    "# Optimiser and learning rate\n",
    "#optimizer = keras.optimizers.SGD\n",
    "#learningrate = 0.01   # SGD default value\n",
    "optimizer = keras.optimizers.Nadam\n",
    "learningrate = 0.001   # SGD default value\n",
    "# Set size of batch and number of epochs\n",
    "batch_size = 32\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback - this is executed when fitting and will \n",
    "# stop and restore best result\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                                  patience=5, \n",
    "                                                  restore_best_weights=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## 5. Build a simple Convolutional Neural Network\n",
    "\n",
    "We will use the keras version built into tensorflow version 2.\n",
    "It is remarkably simple for building, training and evaluating networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valacc, testacc, history, model = do_all(hiddensizes, actfn, optimizer, learningrate, n_train, n_valid, n_epochs, batch_size)\n",
    "model = model_cnn_factory(hiddensizes, actfn, optimizer, learningrate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate the model\n",
    "valacc, testacc, history, model = fit_evaluate(model, n_train, n_valid, n_epochs, batch_size, \n",
    "                                               callbacks=early_stopping_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the training process\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run the model on the test set and get results (loss and accuracy \n",
    "# both reported)\n",
    "testres = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(testres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can inspect the output class predictions\n",
    "y_pred = np.argmax(model.predict(X_test[:3]), axis=-1)  \n",
    "    # use the first three test cases as an example\n",
    "print(y_pred)   # predicted classes\n",
    "print(class_names[y_pred])   # names of these classes (prediction)\n",
    "print(class_names[y_test[:3]])   # names of true classes\n",
    "# Display an image of the first test sample\n",
    "plt.imshow(X_test[0].reshape((28,28)), cmap=\"gray\")\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also look at the probability of predicting each class rather than the class with max probability\n",
    "# Each row has ten probabilities (one per class)\n",
    "y_proba = model.predict(X_test[:3])\n",
    "print(y_proba.round(2))  # round to two decimal places when printing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## 6. Build a small ResNet and run on same data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation example of Geron: Hands On Machine Learning 2019, Chapter 14\n",
    "class ResidualUnit (keras.layers.Layer): \n",
    "    def __init__ (self, filters, strides=1, activation=\"relu\", **kwargs): \n",
    "        super(). __init__ (**kwargs) \n",
    "        self.activation = keras.activations.get(activation) \n",
    "        self.main_layers = [ \n",
    "            keras.layers.Conv2D(filters, 3, strides=strides, \n",
    "                                padding=\"same\", use_bias=False), \n",
    "            keras.layers.BatchNormalization(), \n",
    "            self.activation, \n",
    "            keras.layers.Conv2D(filters, 3, strides=1, \n",
    "                                padding=\"same\", use_bias=False), \n",
    "            keras.layers.BatchNormalization()] \n",
    "        self.skip_layers = [] \n",
    "        if strides > 1: \n",
    "            self.skip_layers = [ \n",
    "                keras.layers.Conv2D(filters, 1, strides=strides, \n",
    "                                    padding=\"same\", use_bias=False), \n",
    "                keras.layers.BatchNormalization()] \n",
    "            \n",
    "    def call(self, inputs): \n",
    "        Z = inputs \n",
    "        for layer in self.main_layers: \n",
    "            Z = layer(Z) \n",
    "        skip_Z = inputs \n",
    "        for layer in self.skip_layers: \n",
    "            skip_Z = layer(skip_Z) \n",
    "        return self.activation(Z + skip_Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model manually, because we want to take layers into our own hands \n",
    "#(This can be wrapped into a function later for automation.)\n",
    "initial_filters=32\n",
    "residual_unit_filters=[32, 64, 128]\n",
    "\n",
    "model = keras.models.Sequential() \n",
    "model.add(keras.layers.Conv2D(initial_filters, 7, strides=2, \n",
    "                              input_shape=[28, 28, 1], \n",
    "                              padding=\"same\", use_bias=False))\n",
    "model.add(keras.layers.BatchNormalization()) \n",
    "model.add(keras.layers.Activation(actfn)) \n",
    "model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\")) \n",
    "\n",
    "prev_filters = initial_filters\n",
    "for filters in residual_unit_filters: \n",
    "    if filters == prev_filters: \n",
    "        strides = 1\n",
    "    else:\n",
    "        strides = 2\n",
    "    model.add(ResidualUnit(filters, strides=strides)) \n",
    "    prev_filters = filters \n",
    "    \n",
    "model.add(keras.layers.GlobalAvgPool2D()) \n",
    "model.add(keras.layers.Flatten()) \n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer=optimizer(learning_rate=learningrate), \n",
    "              metrics=[\"accuracy\"])   \n",
    "\n",
    "# Output the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate the model\n",
    "valacc, testacc, history, model = fit_evaluate(model, n_train, n_valid, \n",
    "                                               n_epochs, batch_size, callbacks=early_stopping_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the training process\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run the model on the test set and get results (loss and accuracy both reported)\n",
    "testres = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(testres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can inspect the output class predictions\n",
    "y_pred = np.argmax(model.predict(X_test[:3]), axis=-1)  # use the first three test cases as an example\n",
    "print(y_pred)   # predicted classes\n",
    "print(class_names[y_pred])   # names of these classes (prediction)\n",
    "print(class_names[y_test[:3]])   # names of true classes\n",
    "# Display an image of the first test sample\n",
    "plt.imshow(X_test[0].reshape((28,28)), cmap=\"gray\")\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also look at the probability of predicting each class rather than the class with max probability\n",
    "# Each row has ten probabilities (one per class)\n",
    "y_proba = model.predict(X_test[:3])\n",
    "print(y_proba.round(2))  # round to two decimal places when printing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## 7. Your Task\n",
    "\n",
    "- Filter size: change the ResNet architecture to a series of 2 Residual Units with same filter size of 8, then 16, 32, 64, 128. Which one appears best?\n",
    "- Number of residual units: use 32 filters per RU, change the number of RUs from 1 to 2,3,4,5,6. Which one appears best?\n",
    "\n",
    "Write your answers and findings below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
