{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE1LJqODuDP3"
      },
      "source": [
        "# Workshop 9 - Introduction to Convolutional Neural Networks\n",
        "\n",
        "Code for workshop 9.  This will use Keras (within tensorflow v2) to build a small CNN\n",
        "\n",
        "Modified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_Otq68YuDP4"
      },
      "outputs": [],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os, time\n",
        "import pandas as pd\n",
        "\n",
        "# Deep Learning imports\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# To plot nice figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "import seaborn as sns; sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEFNNEX6uDQA"
      },
      "outputs": [],
      "source": [
        "# Check the versions are OK (both should be 2 or more)\n",
        "print(tf.__version__)\n",
        "print(keras.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5mucBwhuDQA"
      },
      "source": [
        "## Load data\n",
        "We will use fashion MNIST, which is a set of small images (28x28) that contain 10 different fashion items - see below for class names and an example image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AT20bT5IuDQA"
      },
      "outputs": [],
      "source": [
        "# This is a built-in data for keras, so easily accessible\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvD4dQZMuDQA"
      },
      "outputs": [],
      "source": [
        "# Let's see how big it is\n",
        "print(X_train_full.shape)\n",
        "print(X_test.shape)\n",
        "n_total = X_train_full.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUpjfpbBuDQB"
      },
      "outputs": [],
      "source": [
        "# Scale the data appropriately (it starts with max of 255, but we want max of 1)\n",
        "# We will do this \"by hand\" here, but we could build a pipeline scaler for this instead\n",
        "# We also split the training set given to us into training and validation subsets\n",
        "#   The value of 5000 samples as the size of the validation set is an arbitrary choice\n",
        "X_test = X_test/255.0\n",
        "X_valid, X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0 \n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "class_names = np.array([ \"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\" ])\n",
        "\n",
        "# Inspect some aspects of the data (in general, you should play around with the data \n",
        "#                                   more than this to get a feel for it)\n",
        "# Check that scaled types are appropriate\n",
        "print(X_train.dtype)\n",
        "print(X_valid.dtype)\n",
        "# Look at first item\n",
        "print(class_names[y_train[0]])\n",
        "plt.imshow(X_train[0,:,:], cmap='gray')\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWf7YRWouDQB"
      },
      "outputs": [],
      "source": [
        "# Let's look at the distribution of labels in the training, validation and test sets\n",
        "plt.hist(y_train)\n",
        "plt.show()\n",
        "plt.hist(y_valid)\n",
        "plt.show()\n",
        "plt.hist(y_test)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZzeJv8HuDQB"
      },
      "source": [
        "## Simple Neural Network Code with Keras\n",
        "\n",
        "We will use the keras version built into tensorflow version 2.\n",
        "It is remarkably simple for building, training and evaluating networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Dx6Cm1QuDQB"
      },
      "outputs": [],
      "source": [
        "# Some key parameters\n",
        "n_train = 3000\n",
        "n_valid = 1000\n",
        "# Define the number and size of hidden layers\n",
        "hiddensizes = [16, 32, 16]\n",
        "# Define the activation function to be used by hidden layers\n",
        "actfn = \"relu\"\n",
        "# Optimiser and learning rate\n",
        "optimizer = keras.optimizers.SGD\n",
        "learningrate = 0.01   # SGD default value\n",
        "# Set size of batch and number of epochs\n",
        "batch_size = 32\n",
        "n_epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ckmv_nT-uDQC"
      },
      "outputs": [],
      "source": [
        "# Build a CNN\n",
        "def model_cnn_factory(hiddensizes, actfn, optimizer, learningrate):\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.Conv2D(filters=hiddensizes[0], kernel_size=3, strides=1, activation=actfn, padding=\"same\", \n",
        "                                  input_shape=[28, 28, 1]))    # input layer goes into this 2D convolution\n",
        "    model.add(keras.layers.MaxPooling2D(pool_size=2))          # Pool (downsize)\n",
        "    # for n in hiddensizes[1:-1]:\n",
        "    #     model.add(keras.layers.Conv2D(filters=n, kernel_size=3, strides=1, padding=\"same\", activation=actfn))  # 2nd Conv\n",
        "    #     model.add(keras.layers.MaxPooling2D(pool_size=2))          # Pool (downsize)\n",
        "    model.add(keras.layers.Conv2D(filters=16, kernel_size=3, strides=1, padding=\"same\", activation=actfn))  # 2nd Conv\n",
        "    model.add(keras.layers.MaxPooling2D(pool_size=2))          # Pool (downsize)\n",
        "    model.add(keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding=\"same\", activation=actfn))  # 2nd Conv\n",
        "    model.add(keras.layers.MaxPooling2D(pool_size=2))          # Pool (downsize)\n",
        "    model.add(keras.layers.Conv2D(filters=16, kernel_size=3, strides=1, padding=\"same\", activation=actfn))  # 2nd Conv\n",
        "    model.add(keras.layers.MaxPooling2D(pool_size=2))          # Pool (downsize)\n",
        "\n",
        "    model.add(keras.layers.Conv2D(filters=hiddensizes[-1], kernel_size=3, strides=1, padding=\"same\", activation=actfn))  # 2nd Conv\n",
        "    model.add(keras.layers.Flatten())                          # unravel into a 1D vector\n",
        "    model.add(keras.layers.Dense(10, activation = \"softmax\"))  # always have 10 classes\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer(lr=learningrate), metrics=[\"accuracy\"])   \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xP9d16Y_uDQC"
      },
      "outputs": [],
      "source": [
        "# Optional aside: the CNN can become a Fully Convolutional Network (FCN) by replacing the Flatten and Dense lines with\n",
        "#   model.add(keras.layers.Conv2D(filters=10, kernel_size=7, padding=\"valid\", activation=\"softmax\"))\n",
        "# This uses a kernel equal to the full image size (at this point) to generate a single output per filter \n",
        "#  which requires the convolution to be \"valid\" and not \"same\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_58bd5OuDQD"
      },
      "outputs": [],
      "source": [
        "def model_dense_factory(hiddensizes, actfn, optimizer, learningrate):\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.Flatten(input_shape = [28, 28, 1]))    # always have same sized inputs\n",
        "    for n in hiddensizes:\n",
        "        model.add(keras.layers.Dense(n, activation = actfn))\n",
        "    model.add(keras.layers.Dense(10, activation = \"softmax\"))   # always have 10 classes\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer(lr=learningrate), metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_bCo6W3uDQD"
      },
      "outputs": [],
      "source": [
        "# Reshape the data to be shape [Nx, Ny, 1]  (previously 2D was fine, but for CNN we need depth too)\n",
        "X_train = X_train.reshape((-1, 28, 28, 1))\n",
        "X_valid = X_valid.reshape((-1, 28, 28, 1))\n",
        "X_test = X_test.reshape((-1, 28, 28, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PjIbTXKuDQD"
      },
      "outputs": [],
      "source": [
        "# Early stopping callback - this is executed when fitting and will stop and restore best result\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnFNtbG9uDQE"
      },
      "outputs": [],
      "source": [
        "def do_all(hiddensizes, actfn, optimizer, learningrate, n_train, n_valid, n_epochs, batch_size):\n",
        "    model = model_cnn_factory(hiddensizes, actfn, optimizer, learningrate)\n",
        "    history = model.fit(X_train[:n_train,:,:,:], y_train[:n_train], epochs=n_epochs, callbacks = [early_stopping_cb],\n",
        "                        validation_data=(X_valid[:n_valid,:,:,:], y_valid[:n_valid]))\n",
        "    max_val_acc = np.max(history.history['val_accuracy'])\n",
        "    testres = model.evaluate(X_test, y_test, verbose=0)\n",
        "    return (max_val_acc, testres[1], history, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5KgRiyjuDQE"
      },
      "outputs": [],
      "source": [
        "valacc, testacc, history, model = do_all(hiddensizes, actfn, optimizer, learningrate, n_train, n_valid, n_epochs, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PU9Wt9ROuDQE"
      },
      "outputs": [],
      "source": [
        "def plot_history(history):\n",
        "    # Plot the results (shifting validation curves appropriately)\n",
        "    plt.figure(figsize=(8,5))\n",
        "    n = len(history.history['accuracy'])\n",
        "    plt.plot(np.arange(0,n),history.history['accuracy'], color='orange')\n",
        "    plt.plot(np.arange(0,n),history.history['loss'],'b')\n",
        "    plt.plot(np.arange(0,n)+0.5,history.history['val_accuracy'],'r')  # offset both validation curves\n",
        "    plt.plot(np.arange(0,n)+0.5,history.history['val_loss'],'g')\n",
        "    plt.legend(['Train Acc','Train Loss','Val Acc','Val Loss'])\n",
        "    plt.grid(True)\n",
        "    plt.gca().set_ylim(0, 1) # set the vertical range to [0-1] \n",
        "    plt.show() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er5AbNimuDQE"
      },
      "outputs": [],
      "source": [
        "# We can inspect the output class predictions\n",
        "y_pred = model.predict_classes(X_test[:3])  # use the first three test cases as an example\n",
        "print(y_pred)   # predicted classes\n",
        "print(class_names[y_pred])   # names of these classes (prediction)\n",
        "print(class_names[y_test[:3]])   # names of true classes\n",
        "# Display an image of the first test sample\n",
        "plt.imshow(X_test[0].reshape((28,28)), cmap=\"gray\")\n",
        "plt.grid(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI2IQFzEuDQF"
      },
      "outputs": [],
      "source": [
        "# Now run the model on the test set and get results (loss and accuracy both reported)\n",
        "testres = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(testres)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdsXEw_UuDQF"
      },
      "outputs": [],
      "source": [
        "# We can also look at the probability of predicting each class rather than the class with max probability\n",
        "# Each row has ten probabilities (one per class)\n",
        "y_proba = model.predict(X_test[:3])\n",
        "print(y_proba.round(2))  # round to two decimal places when printing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWkp4qFsuDQF"
      },
      "source": [
        "# Exploring parameters\n",
        "\n",
        "For example, why make these particular choices for architecture and parameters.\n",
        "\n",
        "Let us systematically vary one parameter at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYimn2-CuDQF"
      },
      "outputs": [],
      "source": [
        "# Learning rate\n",
        "res=[]\n",
        "for lr in [100, 10, 1, 0.1]:\n",
        "    valacc, testacc, history = do_all(hiddensizes, actfn, optimizer, lr*learningrate, n_train, n_valid, n_epochs, batch_size)\n",
        "    plot_history(history)\n",
        "    res += [[lr*learningrate,valacc,testacc]]\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8ELtaHKuDQG"
      },
      "outputs": [],
      "source": [
        "# Number of layers\n",
        "res=[]\n",
        "for n in [1, 2, 3]:\n",
        "    valacc, testacc, history = do_all(hiddensizes[:n], actfn, optimizer, learningrate, n_train, n_valid, n_epochs, batch_size)\n",
        "    plot_history(history)\n",
        "    res += [[n,valacc,testacc]]\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0bgz-eouDQG"
      },
      "outputs": [],
      "source": [
        "# Training set size\n",
        "res=[]\n",
        "for ntr in [n_train*0.01, n_train*0.1, n_train*0.5, n_train]:\n",
        "    valacc, testacc, history = do_all(hiddensizes, actfn, optimizer, learningrate, int(ntr), n_valid, n_epochs, batch_size)\n",
        "    plot_history(history)\n",
        "    res += [[int(ntr),valacc,testacc]]\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sISH8cseuDQG"
      },
      "outputs": [],
      "source": [
        "# Plot results (test accuracy vs factor that is changing)\n",
        "res=np.array(res)\n",
        "plt.plot(res[:,0],res[:,2],'-o')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZoYlWWAuDQG"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "workshop09.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}