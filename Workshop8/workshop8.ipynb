{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 8 - Introduction to Deep Neural Networks\n",
    "\n",
    "Code for workshop 8.  This will use Keras (within tensorflow v2) to build a small fully connected network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os, time\n",
    "import pandas as pd\n",
    "\n",
    "# Our new Deep Learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# To plot nice figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the versions are OK (both should be 2 or more)\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "We will use fashion MNIST, which is a set of small images (28x28) that contain 10 different fashion items - see below for class names and an example image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a built-in data for keras, so easily accessible\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how big it is\n",
    "print(X_train_full.shape)\n",
    "print(X_test.shape)\n",
    "n_total = X_train_full.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data appropriately (it starts with max of 255, but we want max of 1)\n",
    "# We will do this \"by hand\" here, but we could build a pipeline scaler for this instead\n",
    "# We also split the training set given to us into training and validation subsets\n",
    "#   The value of 5000 samples as the size of the validation set is an arbitrary choice\n",
    "X_test = X_test/255.0\n",
    "X_valid, X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0 \n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "class_names = np.array([ \"T-shirt/top\" , \"Trouser\" , \"Pullover\" , \"Dress\" , \"Coat\" , \"Sandal\" , \n",
    "               \"Shirt\" , \"Sneaker\" , \"Bag\" , \"Ankle boot\" ])\n",
    "\n",
    "# Inspect some aspects of the data (in general, you should play around with the data \n",
    "#                                   more than this to get a feel for it)\n",
    "# Check that scaled types are appropriate\n",
    "print(X_train.dtype)\n",
    "print(X_valid.dtype)\n",
    "# Look at first item\n",
    "print(class_names[y_train[0]])\n",
    "plt.imshow(X_train[0,:,:], cmap='gray')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the distribution of labels in the training, validation and test sets\n",
    "plt.hist(y_train)\n",
    "plt.show()\n",
    "plt.hist(y_valid)\n",
    "plt.show()\n",
    "plt.hist(y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network Code with Keras\n",
    "\n",
    "We will use the keras version built into tensorflow version 2.\n",
    "It is remarkably simple for building, training and evaluating networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is *all* the code needed to define the network architecture\n",
    "#  1 input layer + 2 hidden layers of sizes 300 and 100 + 1 output layer\n",
    "model = keras.models.Sequential()   # This is a style of building networks - the easiest option\n",
    "model.add(keras.layers.Flatten(input_shape = [28 , 28]))   # our inputs are 28 x 28 arrays, so need to become 1D\n",
    "model.add(keras.layers.Dense(300, activation = \"relu\"))    # first hidden layer\n",
    "model.add(keras.layers.Dense(100, activation = \"relu\"))    # second hidden layer\n",
    "model.add(keras.layers.Dense(10, activation = \"softmax\"))  # output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out some info on the network - take note of the number of parameters (weights and biases)\n",
    "# Can you calculate the number of parameters yourself?\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Couple the architecture definition to a loss function and optimiser (and other performance metrics)\n",
    "# Note that we use sparse_categorical_crossentropy because the entries in y_train are just numbers from 0 to 9\n",
    "# If y_train was encoded as one-hot vectors, then the appropriate loss would be categorical_crossentropy instead\n",
    "# We will use the SGD optimiser here, but Adam is another popular choice\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train (fit) the model, specifying appropriate data and number of epochs (small here to save time)\n",
    "# Save the history of metrics versus epochs - *always* keep this\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple way to plot the history information (see later for more detail)\n",
    "pd.DataFrame(history.history).plot(figsize=(8 , 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run the model on the test set and get results (loss and accuracy both reported)\n",
    "testres = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(testres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can inspect the output class predictions\n",
    "y_pred = model.predict_classes(X_test[:3])  # use the first three test cases as an example\n",
    "print(y_pred)   # predicted classes\n",
    "print(class_names[y_pred])   # names of these classes (prediction)\n",
    "print(class_names[y_test[:3]])   # names of true classes\n",
    "# Display an image of the first test sample\n",
    "plt.imshow(X_test[0].reshape((28,28)), cmap=\"gray\")\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also look at the probability of predicting each class rather than the class with max probability\n",
    "# Each row has ten probabilities (one per class)\n",
    "y_proba = model.predict(X_test[:3])\n",
    "print(y_proba.round(2))  # round to two decimal places when printing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now for a little more detail\n",
    "\n",
    "For example, why make these particular choices for architecture and parameters.\n",
    "\n",
    "We will set up some more general code so that we can build and compare a range of networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More general settings\n",
    "# Let's allow the size of the training and validation sets to change\n",
    "#n_train = n_total   # as above\n",
    "#n_valid = 5000      # as above\n",
    "n_train = 5000   # smaller set\n",
    "n_valid = 1000\n",
    "# Define the number and size of hidden layers\n",
    "#hiddensizes = [300, 100]  # this was what we did above\n",
    "hiddensizes = [100]   # instead, try a simpler one (only one layer)\n",
    "# Define the activation function to be used by hidden layers\n",
    "actfn = \"relu\"\n",
    "# Optimiser and learning rate\n",
    "optimizer = keras.optimizers.SGD\n",
    "learningrate = 0.01\n",
    "# Set size of batch and number of epochs\n",
    "batch_size = 50\n",
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the distribution of labels in the training, validation and test sets we will use\n",
    "plt.hist(y_train[:n_train])\n",
    "plt.show()\n",
    "plt.hist(y_valid[:n_valid])\n",
    "plt.show()\n",
    "plt.hist(y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a more general model using various settings\n",
    "def model_factory(hiddensizes, actfn, optimizer, learningrate):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape = [28 , 28]))    # always have same sized inputs\n",
    "    for n in hiddensizes:\n",
    "        model.add(keras.layers.Dense(n, activation = actfn))\n",
    "    model.add(keras.layers.Dense(10, activation = \"softmax\"))   # always have 10 classes\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer(lr=learningrate), metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our model\n",
    "model = model_factory(hiddensizes, actfn, optimizer, learningrate)\n",
    "# Show summary info\n",
    "model.summary()\n",
    "# Train it with only n_train training samples and n_valid validation samples, and other settings specified above\n",
    "history = model.fit( ### you should fill this in yourself ### )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results (shifting validation curves appropriately)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(np.arange(0,n_epochs),history.history['accuracy'], color='orange')\n",
    "plt.plot(np.arange(0,n_epochs),history.history['loss'],'b')\n",
    "plt.plot(np.arange(0,n_epochs)+0.5,history.history['val_accuracy'],'r')  # offset both validation curves\n",
    "plt.plot(np.arange(0,n_epochs)+0.5,history.history['val_loss'],'g')\n",
    "plt.legend(['Train Acc','Train Loss','Val Acc','Val Loss'])\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1] \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best accuracy on validation set\n",
    "max_val_acc = np.max(history.history['val_accuracy'])\n",
    "print(f'Best validation accuracy = {max_val_acc:.3f}')   # NB: .3f displays 3 decimal places\n",
    "# Evaluate on testset\n",
    "testres = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Accuracy on test set = {testres[1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_all(hiddensizes, actfn, optimizer, learningrate, n_train, n_valid, n_epochs, batch_size):\n",
    "    # Create our model\n",
    "    model = model_factory( ### fill this in yourself ### )\n",
    "    # Train it\n",
    "    history = model.fit( ### fill this in yourself ### )\n",
    "    max_val_acc = np.max(history.history['val_accuracy'])\n",
    "    testres = model.evaluate( ### fill this in yourself ### )\n",
    "    return (max_val_acc, testres[1], history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_model():\n",
    "    # Make some random selections to see how performance varies with network size and training set size\n",
    "    lsize=[10,100,1000]   # just have three options for layer sizes\n",
    "    # start with three candidate layers\n",
    "    hiddensizes = [lsize[np.random.randint(0,3)], lsize[np.random.randint(0,3)], lsize[np.random.randint(0,3)]]\n",
    "    # then keep between 1 and 3 layers\n",
    "    hiddensizes = hiddensizes[:np.random.randint(1,4)]\n",
    "    n_train = np.random.randint(1,10)*10**np.random.randint(2,4)  # numbers between 100 and 9000 (logarithmically selected)\n",
    "    n_epochs = 10  # to speed things up a bit (not recommended for full evaluation)\n",
    "    # Now build, run and report\n",
    "    valacc, testacc, history = do_all(hiddensizes, actfn, optimizer, learningrate, n_train, n_valid, n_epochs, batch_size)\n",
    "    print(f'Settings are: {(hiddensizes, actfn, optimizer, learningrate, n_train, n_valid, n_epochs, batch_size)}')\n",
    "    print(f'Validation accuracy = {valacc:.3f} and Test accuracy = {testacc:.3f}')\n",
    "    retval = [hiddensizes, n_train, valacc, testacc]\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = random_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
